{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Matthew Sit\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "import os\n",
    "from noisyopt import minimizeCompass\n",
    "from scipy.optimize import minimize\n",
    "from heapq import heappush, heappop\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import pickle\n",
    "from operator import itemgetter\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import csv\n",
    "import h5py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters that can be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Files\n",
    "opt_path = './optimization_results_path/opt_result_single_stim_without_sensitivity_passive.hdf5'\n",
    "params_path = '../run_volts_bbp_passive/params/params_bbp_passive.hdf5'\n",
    "score_path = '../scores/'\n",
    "params_file = h5py.File(params_path, 'r')\n",
    "opt_file = h5py.File(opt_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dx = params_file['dx'][0]\n",
    "nModelParams = params_file['param_num'][0]\n",
    "coarseStepSize = 0.1\n",
    "fineStepSize = 0.01\n",
    "\n",
    "# Value in range [0, 1] reflecting percent of data to use for training.\n",
    "# 1 - proportionToTrain will be used for validation.\n",
    "proportionToTrain = 0.7\n",
    "\n",
    "# Integer in range [1, len(stim_names)].\n",
    "# The number of top stims to use for multi-stim optimization.\n",
    "# If k = 1, then use the top stim only.\n",
    "k = 20\n",
    "\n",
    "# random seed to use for train/validation on optimization\n",
    "seed = 500\n",
    "\n",
    "# Weights for optimization.\n",
    "# obj_comb_vec is vector of weights for [spearman, mean, standard deviation].\n",
    "# In general, we want to maximize spearman, maximize mean, and minimize standard deviation.\n",
    "obj_comb_vec = [1, 0]\n",
    "\n",
    "param_select_vec = None\n",
    "\n",
    "score_function_list = [e.decode('ascii') for e in opt_file['ordered_score_function_list'][:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump these parameters to a pickle file so that\n",
    "# they can be used by the utility script file\n",
    "# when it is run in the next section.\n",
    "\n",
    "with open(\"params.pkl\", 'wb') as f:\n",
    "    pickle.dump([\n",
    "        score_path,\n",
    "        params_path,\n",
    "        score_function_list,\n",
    "        obj_comb_vec,\n",
    "        param_select_vec,\n",
    "        dx,\n",
    "        nModelParams,\n",
    "        coarseStepSize,\n",
    "        fineStepSize\n",
    "    ], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params.pkl successfully loaded.\n",
      "params.pkl successfully loaded.\n",
      "Normalizing sensitivity dictionary\n"
     ]
    }
   ],
   "source": [
    "# Load analysis-specific utilities from script file.\n",
    "%run ./new_AnalyzeP.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixes a shuffled order.\n",
    "# Applies the same shuffled order on fed data.\n",
    "# After shuffling, you can use say the first 70% for training and the last 30% for validation.\n",
    "def fix_shuffled_subset_list(length):\n",
    "    shuffle_pattern = np.arange(length)\n",
    "    np.random.shuffle(shuffle_pattern)\n",
    "    def subset_list(subset=None):\n",
    "        if subset is 'train':\n",
    "            return sorted(shuffle_pattern[:int(length*proportionToTrain)])\n",
    "        elif subset is 'test':\n",
    "            return sorted(shuffle_pattern[int(length*proportionToTrain):])\n",
    "        else:\n",
    "            return shuffle_pattern\n",
    "    return subset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the sensitivity matrix as a heat map. \n",
    "# Vertical axis is the parameter index and \n",
    "# Horizontal axis is the numbering of each param set.\n",
    "def plot_heat_map(mat, stim='', sf=''):\n",
    "    # Heat Map for all elementary effects.\n",
    "    #['gna_dend','gna_node','gna_soma','gkv_axon','gkv_soma','gca_dend','gkm_dend','gkca_dend','gca_soma','gkm_soma','gkca_soma','depth_cad']\n",
    "    data = mat.T\n",
    "    plt.imshow(data, cmap='RdBu_r', aspect='auto')\n",
    "    ax = plt.gca()\n",
    "    ax.invert_yaxis()\n",
    "#    if stim is 'Weighted matrix':\n",
    "    #plt.title('Weighted matrix', fontsize=15)\n",
    "#     else:\n",
    "    plt.title('Elementary effect for ' + stim + ' input\\n and ' + sf + ' score function', fontsize=15)\n",
    "    plt.ylabel('Parameter set', fontsize=18)\n",
    "    plt.xlabel('Parameter name', fontsize=18)\n",
    "    plt.colorbar()\n",
    "    #plt.savefig('./heat_map_potassium.eps', format='eps', dpi=1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lin_comb_mat(stim_name, weight_vector, sensitivity_dict):\n",
    "    return sum([sensitivity_dict[stim_name][i]*weight_vector[i] for i in range(len(weight_vector))])\n",
    "\n",
    "def plot_lin_comb_as_heat_map(stim_name, weight_vector, sensitivity_dict):\n",
    "    if isinstance(stim_name, str):\n",
    "        # Single stim.\n",
    "        lin_comb_mat = compute_lin_comb_mat(stim_name, weight_vector, sensitivity_dict)\n",
    "        print('Weighted Mat: ')\n",
    "        plot_heat_map(lin_comb_mat, stim_name)\n",
    "    else:\n",
    "        # Multi-stim.\n",
    "        lin_comb_mat = sum([\n",
    "            compute_lin_comb_mat(stim_name[i], weight_vector[i*len(score_function_list):(i+1)*len(score_function_list)], sensitivity_dict) for i in range(len(stim_name))])\n",
    "\n",
    "        plot_heat_map(lin_comb_mat, '\\n & '.join(stim_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimsInOrder = [e.decode('ascii') for e in opt_file['stims_optimal_order'][:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation on Score Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @arg stim_name can either be the name of a single stim, or it can be a list of multiple stim names.\n",
    "# In the latter case, optimize will be run in multi-stim mode.\n",
    "def trainAndValidateScoreOptimization(stim_name, showHeatMap=False, seed=500, verbosity=True, saveToFile=False):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if verbosity:\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.title('Optimal weighted scores for ' + '\\n & '.join(stim_name))\n",
    "        plt.xlabel('Parameter Set Rank')\n",
    "        plt.ylabel('Optimization value')\n",
    "\n",
    "    if isinstance(stim_name, str):\n",
    "        N = len(pin_score_dict[stim_name][0])\n",
    "    else:\n",
    "        NForEachStim = [len(pin_score_dict[single_stim][0]) for single_stim in stim_name]\n",
    "        # Each of the stims in the list must be of the same length!\n",
    "        assert all(N == NForEachStim[0] for N in NForEachStim), 'Each stim is not of the same length.'\n",
    "        N = NForEachStim[0]\n",
    "    \n",
    "    # Create a pattern and use the pattern to select indices of training and testing data.\n",
    "    shuffle_pattern = fix_shuffled_subset_list(N)\n",
    "    training = shuffle_pattern('train')\n",
    "    testing = shuffle_pattern('test')\n",
    "\n",
    "    # Optimize on the training set.\n",
    "    train_result, train_score_mat, _ = optimize(stim_name, training, obj_comb_vec = obj_comb_vec)\n",
    "    train_result = train_result.x\n",
    "    if verbosity:\n",
    "        plt.scatter(training, train_result @ train_score_mat, label='Training Data')\n",
    "        plt.plot(training, np.poly1d(np.polyfit(training, train_result @ train_score_mat, 1))(training))\n",
    "    \n",
    "    # Optimize on the entire set to establish a ground truth.\n",
    "    test_result, test_score_mat, _ = optimize(stim_name, obj_comb_vec = obj_comb_vec)\n",
    "    # Optimize on the testing set to establish a ground truth.\n",
    "    #test_result, test_score_mat, _ = optimize(stim_name, testing, obj_comb_vec = obj_comb_vec)\n",
    "    test_result = test_result.x\n",
    "    if verbosity:\n",
    "        plt.scatter(np.arange(N), train_result @ test_score_mat, label='Testing Data')\n",
    "        plt.plot(np.arange(N), np.poly1d(np.polyfit(np.arange(N), train_result @ test_score_mat, 1))(np.arange(N)))\n",
    "        #plt.scatter(np.arange(N), test_result @ test_score_mat, label='Ground Truth Test')\n",
    "        #plt.plot(np.arange(N), np.poly1d(np.polyfit(np.arange(N), test_result @ test_score_mat, 1))(np.arange(N)))\n",
    "    \n",
    "        # Replot training and testing data on top of ground truth data.\n",
    "        plt.scatter(np.arange(N), train_result @ test_score_mat, color='C1')\n",
    "        plt.scatter(training, train_result @ train_score_mat, color='C0')\n",
    "    \n",
    "        # Print stims used, which were the top k stims.\n",
    "        if len(stim_name) == 1:\n",
    "            print('Single stim optimization:', stim_name[0])\n",
    "        else:\n",
    "            print('Top', min(k, len(stim_name)), 'stims:', stim_name)\n",
    "        print()\n",
    "        \n",
    "        # Print weights\n",
    "        print('Each row belongs to a single stim.')\n",
    "        print('Training Weights:\\n', train_result.reshape([min(k, len(stim_name)), len(score_function_list)]))\n",
    "        print('Ground Truth Weights:\\n', test_result.reshape([min(k, len(stim_name)), len(score_function_list)]))\n",
    "        print()\n",
    "\n",
    "        # Print spearman scores for the three sets of sampled data.\n",
    "        print('Training Spearman:', round(stat.spearmanr(np.asarray(training), train_result @ train_score_mat)[0], 5))\n",
    "        print('Testing Spearman:', round(stat.spearmanr(np.arange(N), train_result @ test_score_mat)[0], 5))\n",
    "        print('Ground Truth Spearman:', round(stat.spearmanr(np.arange(N), test_result @ test_score_mat)[0], 5))\n",
    "\n",
    "        plt.legend()\n",
    "        #plt.savefig('./trainset_and_groundtruth_potassium.eps', format='eps', dpi=1000)\n",
    "        plt.show()\n",
    "\n",
    "#     if showHeatMap:\n",
    "#         plot_lin_comb_as_heat_map(stim_name, train_result, sensitivity_dict)\n",
    "        \n",
    "#     if saveToFile:\n",
    "#         # Save the ground truth weights reshaped into a single vector.\n",
    "#         #np.savetxt(\"optimal_weight_list.csv\", test_result.reshape([1, test_result.shape[0]]), delimiter=\",\")\n",
    "        \n",
    "#         # Save the stim names used in this optimization.\n",
    "#         with open(\"optimal_stim_list.csv\", 'w') as f:\n",
    "#             wr = csv.writer(f, quoting=csv.QUOTE_ALL, delimiter=\" \")\n",
    "#             wr.writerow(stim_name)\n",
    "        \n",
    "    # Return weights\n",
    "    stims_optimal_order = [np.string_(e) for e in stim_name]\n",
    "    return np.array(test_result.reshape([1, test_result.shape[0]])), np.array(stims_optimal_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list, stim_list = trainAndValidateScoreOptimization(stimsInOrder[:k], True, seed=seed, saveToFile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result_hdf5 = h5py.File('./optimization_results_path/multi_stim_without_sensitivity_bbp_passive_1_0_20_stims.hdf5', 'w')\n",
    "ordered_score_function_list_as_binary = np.array([np.string_(e) for e in score_function_list])\n",
    "opt_result_hdf5.create_dataset('ordered_score_function_list', data=ordered_score_function_list_as_binary)\n",
    "opt_result_hdf5.create_dataset('opt_stim_name_list', data=stim_list)\n",
    "opt_result_hdf5.create_dataset('opt_weight_list', data=weight_list[0])\n",
    "opt_result_hdf5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-.env]",
   "language": "python",
   "name": "conda-env-.conda-.env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
